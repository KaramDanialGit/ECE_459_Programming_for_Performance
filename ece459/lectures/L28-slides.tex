\input{configuration}
\usepackage{soul}

\title{Lecture 28 --- Memory Profiling, Cachegrind }

\author{Jeff Zarnett \\ \small \texttt{jzarnett@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

 \end{frame}

\part{Memory Profiling}
\begin{frame}
\partpage
\end{frame}

\begin{frame}
\frametitle{\st{Memory Profiling} Return to Asgard}

\large

So far: CPU profiling. 

Memory profiling is also a thing; \\
\qquad specifically heap profiling.

``Still Reachable'': not freed \& still have pointers, \\
\qquad but should have been freed?


\end{frame}



\begin{frame}
\frametitle{\st{Memory Profiling} Return to Asgard}

\large

As with queueing theory:\\
\qquad allocs $>$ frees $\Longrightarrow$ usage $\rightarrow \infty$

At least more paging, maybe total out-of-memory.

But! Memory isn't really lost: we could free it.

Our tool for this comes from the Valgrind tool suite.


\end{frame}


\begin{frame}
\frametitle{DHAT}
DHAT tracks allocation points and what happens to them over a program's execution. 

An allocation point is a point in the program which allocates memory.

\end{frame}


\begin{frame}[fragile]
\frametitle{Leaf Node}

{\scriptsize
\begin{verbatim}
AP 1.1.1.1/2 {
  Total:     31,460,928 bytes (2.32%, 1,565.9/Minstr) in 262,171 blocks (4.41%, 13.05/Minstr), 
     avg size 120 bytes, avg lifetime 986,406,885.05 instrs (4.91% of program duration)
  Max:       16,779,136 bytes in 65,543 blocks, avg size 256 bytes
  At t-gmax: 0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  Reads:     5,964,704 bytes (0.11%, 296.88/Minstr), 0.19/byte
  Writes:    10,487,200 bytes (0.51%, 521.98/Minstr), 0.33/byte
  Allocated at {
    ^1: 0x95CACC9: alloc (alloc.rs:72)
      [omitted]
    ^7: 0x95CACC9: parse_token_trees_until_close_delim (tokentrees.rs:27)
    ^8: 0x95CACC9: syntax::parse::lexer::tokentrees::<impl syntax::parse::lexer::
                          StringReader<'a>>::parse_token_tree (tokentrees.rs:81)
    ^9: 0x95CAC39: parse_token_trees_until_close_delim (tokentrees.rs:26)
    ^10: 0x95CAC39: syntax::parse::lexer::tokentrees::<impl syntax::parse::lexer::
                          StringReader<'a>>::parse_token_tree (tokentrees.rs:81)
    #11: 0x95CAC39: parse_token_trees_until_close_delim (tokentrees.rs:26)
    #12: 0x95CAC39: syntax::parse::lexer::tokentrees::<impl syntax::parse::lexer::
                          StringReader<'a>>::parse_token_tree (tokentrees.rs:81)
  }
}
\end{verbatim}
}


\end{frame}


\begin{frame}
\frametitle{Parents}
 Going up the tree, DHAT tells you about all of the allocation points that share a call-stack
prefix. 

The \texttt{\^} before the number indicates that the line is copied from the parent.

A \# indicates that the line is unique to that node.

A sibling of the example above would diverge at the call on line 10.

\end{frame}



\begin{frame}
\frametitle{Shieldmaiden to Thor}

\begin{center}
	\includegraphics[width=\textwidth]{images/Sif.jpg}
\end{center}

\end{frame}



\begin{frame}
\frametitle{Using Massif}

\Large

What does Massif do? 

\begin{itemize}
\item How much heap memory is your program using?
\item How did this happen?
\end{itemize}

Next up: example from Massif docs.



\end{frame}

\begin{frame}[fragile]
\frametitle{Example Allocation Program}


\begin{lstlisting}[language=Rust]
fn g() {
    let a = Vec::<u8>::with_capacity(4000);
    std::mem::forget(a)
}

fn f() {
    let a = Vec::<u8>::with_capacity(2000);
    std::mem::forget(a);
    g()
}

fn main() {

    let mut a = Vec::with_capacity(10);
    for _i in 0..10 {
	a.push(Box::new([0;1000]))
    }
    f();
    g();
}
\end{lstlisting}

\end{frame}

\begin{frame}[fragile]
\frametitle{Send in Sif}

After we compile, run the command:
{\scriptsize
\begin{verbatim}
plam@amqui ~/c/p/l/l/L/alloc> valgrind --tool=massif target/debug/alloc
==406569== Massif, a heap profiler
==406569== Copyright (C) 2003-2017, and GNU GPL'd, by Nicholas Nethercote
==406569== Using Valgrind-3.16.1 and LibVEX; rerun with -h for copyright info
==406569== Command: target/debug/alloc
==406569== 
==406569== 
\end{verbatim}
}
\end{frame}


\begin{frame}
\frametitle{That Was Useful!!!}

\large

What happened? 

\begin{enumerate}
\item The program ran slowly (because Valgrind!)

\item No summary data on the console \\
\hspace*{2em} (like memcheck or helgrind or cachegrind.)
\end{enumerate}

Weird. What we got instead was the file \texttt{massif.out.[PID]}.


\end{frame}


\begin{frame}
\frametitle{Post-Processing}

\Large

\texttt{massif.out.[PID]}:\\
\hspace*{2cm} plain text, sort of readable.

Better: \texttt{ms\_print}.

Which has nothing whatsoever to do with Microsoft. Promise.


\end{frame}


\begin{frame}[fragile]
\frametitle{Post-Processed Output}
{\scriptsize
\begin{verbatim}

    KB
19.71^                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                       #
     |                                                                      :#
     |                                                                      :#
     |                                                                      :#
     |                                                                      :#
   0 +----------------------------------------------------------------------->ki
     0                                                                   111.9
\end{verbatim}
}
\end{frame}


\begin{frame}[fragile]
\frametitle{User Friendly, But Not Useful}

\Large

For a long time, nothing happens, then\ldots kaboom! 

Why? We gave it a trivial program.

We should tell Massif to care more \\
about bytes than CPU cycles,\\
with \verb+--time-unit=B+.

Let's try that.



\end{frame}


\begin{frame}
\frametitle{Valgrind (Memcheck) First}


Run valgrind (Memcheck) first and make it happy \\
before we go into figuring out where heap blocks are going with Massif. 

Okay, what to do with the information from Massif, anyway? 

Easy!
\begin{itemize}
\item Start with peak (worst case scenario) \\ and see where that takes you (if anywhere). 

\item You can probably identify some cases where memory is hanging around unnecessarily. 
\end{itemize}



\end{frame}


\begin{frame}
\frametitle{Places to Look with Massif}

\large
Memory usage climbing over a long period of time, perhaps slowly, but never decreasing---memory filling with junk? 

Large spikes in the graph---why so much allocation and deallocation in a short period?
\end{frame}



\begin{frame}[fragile]
\frametitle{Other Massif-ly Useful Things}

\large
\begin{itemize}
	\item stack allocation (\verb+--stacks=yes+).
	\item children of a process \\ (anything split off with \texttt{fork}) if desired.
	\item low level stuff: if going beyond \texttt{malloc}, \texttt{calloc}, \texttt{new}, etc. and using \texttt{mmap} or \texttt{brk} that is usually missed, can do profiling at page level (\verb+--pages-as-heap=yes+).
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Live Demos}

\large

As is often the case, \\ we have examined the tool on a trivial program. 

Let's see if we can do some\\
 live demos of Massif at work.

\end{frame}

\part{Cachegrind}
\begin{frame}
\partpage
\end{frame}


\begin{frame}
\frametitle{How Would You Know the Difference...}

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/redbluepill.jpg}
\end{center}

This is much more performance oriented than the other two tools. 

 It runs a simulation of how your program interacts with cache and evaluates how your program does on branch prediction.
 

\end{frame}


\begin{frame}
\frametitle{Cachegrind}


 As we discussed earlier, cache misses and branch mispredicts have a huge impact on performance.
 
 Recall that a miss from the fastest cache results in a small penalty (10 cycles).
 
 A miss that requires going to memory requires about 200 cycles. 
 
 A mispredicted branch costs somewhere between 10-30 cycles.


\end{frame}

\begin{frame}
\frametitle{Cachegrind Reporting}

Cachegrind reports data about:
\begin{itemize}
	\item The First Level Instruction Cache (I1) [L1 Instruction Cache]
	\item The First Level Data Cache (D1) [L1 Data Cache]
	\item The Last Level Cache (LL) [L3 Cache].
\end{itemize}

Unlike normal Valgrind operation, you probably want to turn optimizations on.

\end{frame}

\begin{frame}[fragile]
\frametitle{Sim-u-laaaaate!}
{\scriptsize

\begin{verbatim}
jz@Loki:~/ece254$ valgrind --tool=cachegrind --branch-sim=yes ./search

--16559-- warning: L3 cache found, using its data for the LL simulation.
Found at 11 by thread 1 
Found at 22 by thread 3 
==16559== 
==16559== I   refs:      310,670
==16559== I1  misses:      1,700
==16559== LLi misses:      1,292
==16559== I1  miss rate:    0.54%
==16559== LLi miss rate:    0.41%
==16559== 
==16559== D   refs:      114,078  (77,789 rd   + 36,289 wr)
==16559== D1  misses:      4,398  ( 3,360 rd   +  1,038 wr)
==16559== LLd misses:      3,252  ( 2,337 rd   +    915 wr)
==16559== D1  miss rate:     3.8% (   4.3%     +    2.8%  )
==16559== LLd miss rate:     2.8% (   3.0%     +    2.5%  )
==16559== 
==16559== LL refs:         6,098  ( 5,060 rd   +  1,038 wr)
==16559== LL misses:       4,544  ( 3,629 rd   +    915 wr)
==16559== LL miss rate:      1.0% (   0.9%     +    2.5%  )
==16559== 
==16559== Branches:       66,622  (65,097 cond +  1,525 ind)
==16559== Mispredicts:     7,202  ( 6,699 cond +    503 ind)
==16559== Mispred rate:     10.8% (  10.2%     +   32.9%   )

\end{verbatim}
}

\end{frame}

\begin{frame}[fragile]
\frametitle{Optimizations: Enabled!}
{\scriptsize
\begin{verbatim}
jz@Loki:~/ece254$ valgrind --tool=cachegrind --branch-sim=yes ./search

--16618-- warning: L3 cache found, using its data for the LL simulation.
Found at 11 by thread 1 
Found at 22 by thread 3 
==16618== 
==16618== I   refs:      306,169
==16618== I1  misses:      1,652
==16618== LLi misses:      1,286
==16618== I1  miss rate:    0.53%
==16618== LLi miss rate:    0.42%
==16618== 
==16618== D   refs:      112,015  (76,522 rd   + 35,493 wr)
==16618== D1  misses:      4,328  ( 3,353 rd   +    975 wr)
==16618== LLd misses:      3,201  ( 2,337 rd   +    864 wr)
==16618== D1  miss rate:     3.8% (   4.3%     +    2.7%  )
==16618== LLd miss rate:     2.8% (   3.0%     +    2.4%  )
==16618== 
==16618== LL refs:         5,980  ( 5,005 rd   +    975 wr)
==16618== LL misses:       4,487  ( 3,623 rd   +    864 wr)
==16618== LL miss rate:      1.0% (   0.9%     +    2.4%  )
==16618== 
==16618== Branches:       65,827  (64,352 cond +  1,475 ind)
==16618== Mispredicts:     7,109  ( 6,596 cond +    513 ind)
==16618== Mispred rate:     10.7% (  10.2%     +   34.7%   )
\end{verbatim}
}

\end{frame}

\begin{frame}
\frametitle{Results Analysis}


Interesting results: our data and instruction miss rates went down marginally but the branch mispredict rates went up!

Well sort of - there were fewer branches and thus fewer we got wrong as well as fewer we got right. 

So the total cycles lost to mispredicts went down. 

Is this an overall win for the code? Yes. 


\end{frame}
\begin{frame}
\frametitle{Do the Math}

In some cases it's not so clear cut, and we could do a small calculation. 

If we just take a look at the LL misses (4~544 vs 4~487) and assume they take 200 cycles, and the branch miss penalty is 200 cycles?

It went from 908~800 wasted cycles to 897~400. A decrease of 11~400 cycles.

  Repeat for each of the measures and sum them up to determine if things got better overall and by how much.

\end{frame}
\begin{frame}
\frametitle{Cachegrind Detailed Output}

Cachegrind also produces a more detailed output file, titled cachegrind.out.<pid> (the PID in the example is 16618). 

This file is not especially human-readable, but we can ask the associated tool \texttt{cg\_annotate} to break it down for us.

The output is way too big for slides.

\end{frame}
\begin{frame}
\frametitle{Advanced Cachegrind}

Cachegrind is very... verbose... and it can be very hard to come up with useful changes based on what you see... 

Assuming your eyes don't glaze over when you see the numbers. 

Probably the biggest performance impact is last level cache misses (those appear as DLmr or DLmw). 

You might also try to look at the Bcm and Bim (branch mispredictions) to see if you can give some better hints. 
\end{frame}
\begin{frame}
\frametitle{Really Advanced Cachegrind}

Of course, to learn more about how Cachegrind, the manual is worth reading. 

Not that anybody reads manuals anymore... 

Just give it a shot, when you get stuck, google the problem, click the first stack overflow link result...


\end{frame}


\end{document}

